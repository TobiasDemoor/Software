## Gradient descent
Trabajando sobre la última neurona de una red de regresión, con una sola entrada, con _bias_ siempre en 0 y con función de activación lineal.
$$C = e^2 = \frac {(y-t)^2}2\ ;\ b_l = 0\ \forall l\ ; \ \varphi(v)=v\ ; \ y = w_l y_{l-1}$$
$$ W^{(n+1)} = W^{(n)} - \eta \nabla C$$
$$ w_l^{(n+1)} = w_l^{(n)} - \eta \frac{\partial C}{\partial w_l^{(n)}}$$
$$\text {Por regla de la cadena: } \frac{\partial C}{\partial w_l^{(n)}} = \frac{\partial C}{\partial y}\frac{\partial y}{\partial w_l^{(n)}}$$Donde:
$$\frac{\partial C}{\partial y} = y - t \ ; \ \frac{\partial y}{\partial w_l^{(n)}} =  y_{l-1}$$
Ahora si trabajamos sobre la anteúltima neurona con una sola entrada.
$$ w_{l-1}^{(n+1)} = w_{l-1}^{(n)} - \eta \frac{\partial C}{\partial w_{l-1}^{(n)}}$$
$$\frac{\partial C}{\partial w_{l-1}^{(n)}}=\frac{\partial C}{\partial y}\frac{\partial y}{\partial y_{l-1}}\frac{\partial y_{l-1}}{\partial w_{l-1}^{(n)}}$$
Donde:
$$\frac{\partial C}{\partial y} = y - t \ ; \ \frac{\partial y}{\partial y_{l-1}} = w_l \ ;\ \frac{\partial y_{l-1}}{\partial w_{l-1}^{(n)}} = y_{l-2}$$
Ahora sobre la antepenúltima.
$$\frac{\partial C}{\partial w_{l-2}^{(n)}}=\frac{\partial C}{\partial y}\frac{\partial y}{\partial y_{l-1}}\frac{\partial y_{l-1}}{\partial y_{l-2}}\frac{\partial y_{l-2}}{\partial w_{l-2}^{(n)}}$$Donde:
$$\frac{\partial C}{\partial y} = y - t \ ; \ \frac{\partial y}{\partial y_{l-1}} = w_l \ ;\ \frac{\partial y_{l-1}}{\partial y_{l-2}}=w_{l-1}\ ;\ \frac{\partial y_{l-2}}{\partial w_{l-2}^{(n)}} = y_{l-3}$$
En la práctica para evitar el costo computacional de realizar todo el proceso para cada valor de training lo que se hace es calcular los gradientes para un subconjunto de los datos (_minibatch_), estos se promedian y utilizando este valor promedio se ajustan los pesos.

## Batch normalization
Se calcula la media $\mu$ y el desvío estándar $\sigma$ en cada _minibatch_ para cada variable, antes de la entrada de la capa siguiente y se normaliza.
$$x_{norm} = \frac {x-\mu}{\sigma} $$
Estas capas con BN aprenden dos parámetros \[$\beta$, $\gamma$].
La activación de la capa será (entrda a la capa siguiente) antes de la alinealidad $\varphi(v)$:
$$v_{norm} = \gamma x_{norm} + \beta$$

# Deep Learning
## Convolutional Neural Networks (CNN)
Partimos de un tensor que representa una imagen y a partir de operaciones sobre este queremos llegar a un *embedding* (o espacio latente) que pueda ser la entrada de una red neuronal.

Se realizan K convoluciones sobre la imagen y las matrices de salida se almacenan en un tensor. Luego se pueden aplicar convoluciones sobre este tensor resultante y así en varias etapas.

Luego de estas etapas se realiza un *pooling* sobre cada capa del tensor y así se obtiene un tensor reducido.

Esto así se puede repetir varias veces, pasando de convoluciones a pooling hasta llegar a un tensor final que aplana y pasa a ser nuestro *embedding* de entrada y en cualquier momento de este proceso se puede pasar por una función de activación (esto es para agregar una alinealidad).

### 2D Convolution
Se hace la convolución de la imágen (I) con un filtro o *kernel* (F, este tiene la misma profundidad que I pero es menor en las otras dos dimensiones).
$$w_{111}x_{111}+w_{112}x_{112}+\dots+b = v $$
Luego de realizar la convolución se genera una matriz (no un tensor) de menor dimensión que el tensor original la reducción del tamaño depende del tamaño original, del *stride*, del tamaño del filtro y si hay *padding* cuánto.
$$M_0 = \frac{M-F-2P}{S}$$

```
Hiperparámetros: F, P, S
```

### Pooling
El *pooling* recibe una matriz como entrada y con un filtro de cierto tamaño obtiene un promedio de los elementos que contiene o el máximo. Como resultado se obtiene una matriz de menor tamaño.

```
Hiperparámetros: F, P, S, max o avg
```

### Regularization
- Layer normalization
- Batch normalization
- Dropout

### Global average pooling
Es una alternativa más drástica al *flatten* donde se devuelve el promedio de cada matríz del tensor previo a ingresarlo en la red neuronal.
# Transformers
Al vector de entrada lo convierto, usando una capa neuronal, en tres vectores (Q, K, V) mediante tres matrices de peso.

El valor que se obtiene es la combinación de la atención para cada token multiplicado por el valor del token.
Por ejemplo el valor para i sería $Attention(q_i*k_j)*v_j$ por cada j combinados.

Este valor que se obtiene es por cada head de atención, donde "se puede prestar atención a distintas cosas". El resultado de todas las heads se concatena y se pasa por una capa neuronal.